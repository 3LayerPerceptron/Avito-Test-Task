# Avito-Test-Task
## Скор
Сабмит полченный с помощью ноутбука solution.ipynb (представлен в виде файла **gt_submission.csv**) получил скор Mean F1 = **80.99%**
## Воспроизводимость
### 1. Локально
Надо просто склонить репозиторий и запустить все ячейки блокнота solution.ipynb, после того как он отработает появится файлик submission.csv
### 2. Google Colab
Необходимо открыть блокнот solution.ipynb в колабе и подгрузить туда два файлика archive.zip и dataset_1937770_3.txt

После этого нужно запустить все ячейки, после исполнения которых появится submission.csv

### Рекомендую запускать локально в колабе блокнот крутится чуть больше 30 минут на cpu. (У меня локально на весь блокнот уходило около 10 минут на cpu)



## Данные полученные из общего доступа

Словарь взят отсюда: https://www.kaggle.com/datasets/rtatman/opencorpora-russian

Модель взята отсюда: https://huggingface.co/kplro/rugpt3small_based_on_gpt2-l2_russian_10_epoch

## Рассуждения (продублировал в readme на всякий)

### Идея 1: взять токенайзер и побить каждый из запросов на токены.

Эта идея сразу отпала, потому что большая часть токенайзеров предполагает, что слова в корпусе будут разделены пробелом.

В добавок, ну, вот побили мы на токены, а как объединять эти токены в слова? (чтобы понять где должны стоять пробелы)

### Идея 2: Накопление Буфера

Давайте отталкиваться от недостатков первой идеи, а именно как объединять эти токены в слова?

Сделаем буфер, куда будем добавлять каждый символ из запроса. Как только мы соберем несколько кандидатов на слова, выберем из них наиболее длинное и поставим после него пробел.

Эта идея подразумевает наличие корпуса, в который мы будем подглядывать, чтобы понять является набор символов словом или нет.

Поэтому я смотался на kaggle и взял OpenCorpora-russian https://www.kaggle.com/datasets/rtatman/opencorpora-russian

Реализацию 2-й идеи можно найти в разделе артефакты **cumulative_imputer_max**.

P.S. Почему выбирается максимальное по длине слово?

Это эвристика, основанная на том, что мы хотим полностью разбить запрос на слова. Т.е. нам в каком-то смысле выгодно откусыввать большие куски запроса.

### Докручиваем идею 2

**Успешные** примеры

отдамдаромкошку -> отдам даром кошку

новыйдивандоставканедорого -> новый диван доставка недорого

**Неудачные** примеры

работавмосквеудаленно -> работав москве удал е н но

сдаюквартирусмебельюитехникой -> сдаю квартиру см е белью ит е х никой

Проблема с неудачными примерами очевидна. Алгоритм работает по эвристике, которая никогда не выберет **работа**, если можно выбрать деепричастие **работав** или **с**, если можно выбрать **см**.

Если мы хотим выбирать комбинации слов, которые наиболее похожи на то, что ввел человек, то нам не избежать использование частот/статистики употребеления того или иного слова.

Вряд ли удастся найти такой список, который полность совпал бы с нашим словарем, поэтому давайте возьмем маленькую языковую модель и будем считать перплексию, вместо максимальной длины слова.

У странных слов по типу **в москве удал е** пералексия будет больше, чем у **в москве удаленно**

Модельку взял с hf https://huggingface.co/kplro/rugpt3small_based_on_gpt2-l2_russian_10_epoch какой-то небольшой чекпоинт, который как раз подходит под наши цели

P.S. Да в идеале обучить что-то свое и поменьше на корпусе объявлений, но как proof-of-concept нам подойдет и эта.

### В разделе код блокнота solution.ipynb представлена реализация докрученной второй идеи (Конкретно в классе Perplexity imputer)
Большая часть кода - чисто техническая перегонка из одного формата в другой и попытка в обработку спец. символов (пунктуация, цифры, знаки препинания и т.д.)
