{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "853dbcc9",
   "metadata": {},
   "source": [
    "## –†–∞—Å—Å—É–∂–¥–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda3d3e5",
   "metadata": {},
   "source": [
    "### –ò–¥–µ—è 1: –≤–∑—è—Ç—å —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä –∏ –ø–æ–±–∏—Ç—å –∫–∞–∂–¥—ã–π –∏–∑ –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ —Ç–æ–∫–µ–Ω—ã.\n",
    "\n",
    "–≠—Ç–∞ –∏–¥–µ—è —Å—Ä–∞–∑—É –æ—Ç–ø–∞–ª–∞, –ø–æ—Ç–æ–º—É —á—Ç–æ –±–æ–ª—å—à–∞—è —á–∞—Å—Ç—å —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–æ–≤ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç, —á—Ç–æ —Å–ª–æ–≤–∞ –≤ –∫–æ—Ä–ø—É—Å–µ –±—É–¥—É—Ç —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –ø—Ä–æ–±–µ–ª–æ–º.\n",
    "\n",
    "–í –¥–æ–±–∞–≤–æ–∫, –Ω—É, –≤–æ—Ç –ø–æ–±–∏–ª–∏ –º—ã –Ω–∞ —Ç–æ–∫–µ–Ω—ã, –∞ –∫–∞–∫ –æ–±—ä–µ–¥–∏–Ω—è—Ç—å —ç—Ç–∏ —Ç–æ–∫–µ–Ω—ã –≤ —Å–ª–æ–≤–∞? (—á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å –≥–¥–µ –¥–æ–ª–∂–Ω—ã —Å—Ç–æ—è—Ç—å –ø—Ä–æ–±–µ–ª—ã)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c62991",
   "metadata": {},
   "source": [
    "### –ò–¥–µ—è 2: –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ –ë—É—Ñ–µ—Ä–∞\n",
    "\n",
    "–î–∞–≤–∞–π—Ç–µ –æ—Ç—Ç–∞–ª–∫–∏–≤–∞—Ç—å—Å—è –æ—Ç –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–æ–≤ –ø–µ—Ä–≤–æ–π –∏–¥–µ–∏, –∞ –∏–º–µ–Ω–Ω–æ –∫–∞–∫ –æ–±—ä–µ–¥–∏–Ω—è—Ç—å —ç—Ç–∏ —Ç–æ–∫–µ–Ω—ã –≤ —Å–ª–æ–≤–∞?\n",
    "\n",
    "–°–¥–µ–ª–∞–µ–º –±—É—Ñ–µ—Ä, –∫—É–¥–∞ –±—É–¥–µ–º –¥–æ–±–∞–≤–ª—è—Ç—å –∫–∞–∂–¥—ã–π —Å–∏–º–≤–æ–ª –∏–∑ –∑–∞–ø—Ä–æ—Å–∞. –ö–∞–∫ —Ç–æ–ª—å–∫–æ –º—ã —Å–æ–±–µ—Ä–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ —Å–ª–æ–≤–∞, –≤—ã–±–µ—Ä–µ–º –∏–∑ –Ω–∏—Ö –Ω–∞–∏–±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω–æ–µ –∏ –ø–æ—Å—Ç–∞–≤–∏–º –ø–æ—Å–ª–µ –Ω–µ–≥–æ –ø—Ä–æ–±–µ–ª.\n",
    "\n",
    "–≠—Ç–∞ –∏–¥–µ—è –ø–æ–¥—Ä–∞–∑—É–º–µ–≤–∞–µ—Ç –Ω–∞–ª–∏—á–∏–µ –∫–æ—Ä–ø—É—Å–∞, –≤ –∫–æ—Ç–æ—Ä—ã–π –º—ã –±—É–¥–µ–º –ø–æ–¥–≥–ª—è–¥—ã–≤–∞—Ç—å, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å —è–≤–ª—è–µ—Ç—Å—è –Ω–∞–±–æ—Ä —Å–∏–º–≤–æ–ª–æ–≤ —Å–ª–æ–≤–æ–º –∏–ª–∏ –Ω–µ—Ç.\n",
    "\n",
    "–ü–æ—ç—Ç–æ–º—É —è —Å–º–æ—Ç–∞–ª—Å—è –Ω–∞ kaggle –∏ –≤–∑—è–ª OpenCorpora-russan https://www.kaggle.com/datasets/rtatman/opencorpora-russian\n",
    "\n",
    "–†–µ–∞–ª–∏–∑–∞—Ü–∏—é 2-–π –∏–¥–µ–∏ –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –≤ —Ä–∞–∑–¥–µ–ª–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã **cumulative_imputer_max**.\n",
    "\n",
    "P.S. –ü–æ—á–µ–º—É –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –ø–æ –¥–ª–∏–Ω–µ —Å–ª–æ–≤–æ?\n",
    "\n",
    "–≠—Ç–æ —ç–≤—Ä–∏—Å—Ç–∏–∫–∞, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ —Ç–æ–º, —á—Ç–æ –º—ã —Ö–æ—Ç–∏–º –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ä–∞–∑–±–∏—Ç—å –∑–∞–ø—Ä–æ—Å –Ω–∞ —Å–ª–æ–≤–∞. –¢.–µ. –Ω–∞–º –≤ –∫–∞–∫–æ–º-—Ç–æ —Å–º—ã—Å–ª–µ –≤—ã–≥–æ–¥–Ω–æ –æ—Ç–∫—É—Å—ã–≤–≤–∞—Ç—å –±–æ–ª—å—à–∏–µ –∫—É—Å–∫–∏ –∑–∞–ø—Ä–æ—Å–∞.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894ecfc8",
   "metadata": {},
   "source": [
    "### –î–æ–∫—Ä—É—á–∏–≤–∞–µ–º –∏–¥–µ—é 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6141bcb7",
   "metadata": {},
   "source": [
    "**–£—Å–ø–µ—à–Ω—ã–µ** –ø—Ä–∏–º–µ—Ä—ã\n",
    "\n",
    "–æ—Ç–¥–∞–º–¥–∞—Ä–æ–º–∫–æ—à–∫—É -> –æ—Ç–¥–∞–º –¥–∞—Ä–æ–º –∫–æ—à–∫—É\n",
    "\n",
    "–Ω–æ–≤—ã–π–¥–∏–≤–∞–Ω–¥–æ—Å—Ç–∞–≤–∫–∞–Ω–µ–¥–æ—Ä–æ–≥–æ -> –Ω–æ–≤—ã–π –¥–∏–≤–∞–Ω –¥–æ—Å—Ç–∞–≤–∫–∞ –Ω–µ–¥–æ—Ä–æ–≥–æ\n",
    "\n",
    "**–ù–µ—É–¥–∞—á–Ω—ã–µ** –ø—Ä–∏–º–µ—Ä—ã\n",
    "\n",
    "—Ä–∞–±–æ—Ç–∞–≤–º–æ—Å–∫–≤–µ—É–¥–∞–ª–µ–Ω–Ω–æ -> —Ä–∞–±–æ—Ç–∞–≤ –º–æ—Å–∫–≤–µ —É–¥–∞–ª –µ –Ω –Ω–æ\n",
    "\n",
    "—Å–¥–∞—é–∫–≤–∞—Ä—Ç–∏—Ä—É—Å–º–µ–±–µ–ª—å—é–∏—Ç–µ—Ö–Ω–∏–∫–æ–π -> —Å–¥–∞—é –∫–≤–∞—Ä—Ç–∏—Ä—É —Å–º –µ –±–µ–ª—å—é –∏—Ç –µ —Ö –Ω–∏–∫–æ–π"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9431f5ff",
   "metadata": {},
   "source": [
    "–ü—Ä–æ–±–ª–µ–º–∞ —Å –Ω–µ—É–¥–∞—á–Ω—ã–º–∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏ –æ—á–µ–≤–∏–¥–Ω–∞. –ê–ª–≥–æ—Ä–∏—Ç–º —Ä–∞–±–æ—Ç–∞–µ—Ç –ø–æ —ç–≤—Ä–∏—Å—Ç–∏–∫–µ, –∫–æ—Ç–æ—Ä–∞—è –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –≤—ã–±–µ—Ä–µ—Ç **—Ä–∞–±–æ—Ç–∞**, –µ—Å–ª–∏ –º–æ–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å –¥–µ–µ–ø—Ä–∏—á–∞—Å—Ç–∏–µ **—Ä–∞–±–æ—Ç–∞–≤** –∏–ª–∏ **—Å**, –µ—Å–ª–∏ –º–æ–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å **—Å–º**.\n",
    "\n",
    "–ï—Å–ª–∏ –º—ã —Ö–æ—Ç–∏–º –≤—ã–±–∏—Ä–∞—Ç—å –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Å–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏ –Ω–∞ —Ç–æ, —á—Ç–æ –≤–≤–µ–ª —á–µ–ª–æ–≤–µ–∫, —Ç–æ –Ω–∞–º –Ω–µ –∏–∑–±–µ–∂–∞—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —á–∞—Å—Ç–æ—Ç/—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —É–ø–æ—Ç—Ä–µ–±–µ–ª–µ–Ω–∏—è —Ç–æ–≥–æ –∏–ª–∏ –∏–Ω–æ–≥–æ —Å–ª–æ–≤–∞.\n",
    "\n",
    "–í—Ä—è–¥ –ª–∏ —É–¥–∞—Å—Ç—Å—è –Ω–∞–π—Ç–∏ —Ç–∞–∫–æ–π —Å–ø–∏—Å–æ–∫, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–ª–Ω–æ—Å—Ç—å —Å–æ–≤–ø–∞–ª –±—ã —Å –Ω–∞—à–∏–º —Å–ª–æ–≤–∞—Ä–µ–º, –ø–æ—ç—Ç–æ–º—É –¥–∞–≤–∞–π—Ç–µ –≤–æ–∑—å–º–µ–º –º–∞–ª–µ–Ω—å–∫—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –∏ –±—É–¥–µ–º —Å—á–∏—Ç–∞—Ç—å –ø–µ—Ä–ø–ª–µ–∫—Å–∏—é, –≤–º–µ—Å—Ç–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã —Å–ª–æ–≤–∞.\n",
    "\n",
    "–£ —Å—Ç—Ä–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤ –ø–æ —Ç–∏–ø—É **–≤ –º–æ—Å–∫–≤–µ —É–¥–∞–ª –µ** –ø–µ—Ä–∞–ª–µ–∫—Å–∏—è –±—É–¥–µ—Ç –±–æ–ª—å—à–µ, —á–µ–º —É **–≤ –º–æ—Å–∫–≤–µ —É–¥–∞–ª–µ–Ω–Ω–æ**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3ce388",
   "metadata": {},
   "source": [
    "–ú–æ–¥–µ–ª—å–∫—É –≤–∑—è–ª —Å hf https://huggingface.co/kplro/rugpt3small_based_on_gpt2-l2_russian_10_epoch –∫–∞–∫–æ–π-—Ç–æ –Ω–µ–±–æ–ª—å—à–æ–π —á–µ–∫–ø–æ–∏–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –∫–∞–∫ —Ä–∞–∑ –ø–æ–¥—Ö–æ–¥–∏—Ç –ø–æ–¥ –Ω–∞—à–∏ —Ü–µ–ª–∏\n",
    "\n",
    "P.S. –î–∞ –≤ –∏–¥–µ–∞–ª–µ –æ–±—É—á–∏—Ç—å —á—Ç–æ-—Ç–æ —Å–≤–æ–µ –∏ –ø–æ–º–µ–Ω—å—à–µ –Ω–∞ –∫–æ—Ä–ø—É—Å–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–π, –Ω–æ –∫–∞–∫ proof-of-concept –Ω–∞–º –ø–æ–¥–æ–π–¥–µ—Ç –∏ —ç—Ç–∞."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5efee3",
   "metadata": {},
   "source": [
    "### –í —Ä–∞–∑–¥–µ–ª–µ –∫–æ–¥ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–æ–∫—Ä—É—á–µ–Ω–Ω–æ–π –≤—Ç–æ—Ä–æ–π –∏–¥–µ–∏ (–ö–æ–Ω–∫—Ä–µ—Ç–Ω–æ –≤ –∫–ª–∞—Å—Å–µ Perplexity imputer)\n",
    "–ë–æ–ª—å—à–∞—è —á–∞—Å—Ç—å –∫–æ–¥–∞ - —á–∏—Å—Ç–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –ø–µ—Ä–µ–≥–æ–Ω–∫–∞ –∏–∑ –æ–¥–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ –≤ –¥—Ä—É–≥–æ–π –∏ –ø–æ–ø—ã—Ç–∫–∞ –≤ –æ–±—Ä–∞–±–æ—Ç–∫—É —Å–ø–µ—Ü. —Å–∏–º–≤–æ–ª–æ–≤ (–ø—É–Ω–∫—Ç—É–∞—Ü–∏—è, —Ü–∏—Ñ—Ä—ã, –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ —Ç.–¥.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f44caf2",
   "metadata": {},
   "source": [
    "## –ö–æ–¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7c2365",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install transformers\n",
    "!pip3 install pandas\n",
    "!pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ca1540",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip archive.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27d333f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418b7111",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"dataset_1937770_3.txt\"\n",
    "VOCAB_PATH = \"dictionary.txt\"\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80109e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(data_path, header=True):\n",
    "    data = []\n",
    "    col_names = []\n",
    "    with open(data_path, 'r') as file:\n",
    "        i = 0\n",
    "        for line in file:\n",
    "            if header:\n",
    "                col_names = line.strip().split(\",\")\n",
    "                header = False\n",
    "                continue\n",
    "            data.append([i, \",\".join(line.split(\",\")[1:]).strip()])\n",
    "            i += 1\n",
    "    return pd.DataFrame(data, columns=col_names)\n",
    "\n",
    "def vocab_loader(vocab_path):\n",
    "    vocab = set()\n",
    "    with open(vocab_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.strip().isnumeric():\n",
    "                continue\n",
    "            vocab.add(line.split(\"\\t\")[0].strip().lower())\n",
    "    vocab.add(\"–±—É\")\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25fce5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vocab_loader(VOCAB_PATH)\n",
    "data_df = data_loader(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82c24479",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"kplro/rugpt3small_based_on_gpt2-l2_russian_10_epoch\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"kplro/rugpt3small_based_on_gpt2-l2_russian_10_epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dcca05d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerplexityImputer():\n",
    "    \n",
    "    def __init__(self, vocab, model, tokenizer):\n",
    "        # parameters\n",
    "        \n",
    "        # storage\n",
    "        self.vocab = vocab\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.letters = set(\"abcdefghijklmnopqrstuvwxyz-‚Äì‚Äô\")\n",
    "        self.digits = set(\"0123456789\")\n",
    "        self.punkt = set(\".,;!?:‚Ä¶\")\n",
    "        return\n",
    "    \n",
    "    def get_next_variant(self, query: str, start: int = 0, amount: int = 1) -> List[List[Tuple[str, int]]]:\n",
    "        \"\"\"\n",
    "        –§—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –≤—ã–¥–∞–µ—Ç amount –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Å–ª–æ–≤ –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö—Å—è —Å –ø–æ–∑–∏—Ü–∏–∏ start\n",
    "\n",
    "        Args:\n",
    "            query  (str) : –∑–∞–ø—Ä–æ—Å.\n",
    "            start  (int) : –ø–æ–∑–∏—Ü–∏—è, —Å –∫–æ—Ç–æ—Ä–æ–π –Ω–∞–¥–æ –Ω–∞—á–∏–Ω–∞—Ç—å –∏—Å–∫–∞—Ç—å —Å–ª–æ–≤–∞.\n",
    "            amount (int) : –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤, –∫–æ—Ç–æ—Ä–æ–µ –Ω—É–∂–Ω–æ –Ω–∞–π—Ç–∏.\n",
    "\n",
    "        Returns:\n",
    "            List[List[Tuple[str, int]]]: –°–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤ –∫–æ—Ä—Ç–µ–∂–µ–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ (—Å–ª–æ–≤–æ, –ø–æ–∑–∏—Ü–∏—è –∏–¥—É—â–∞—è –ø–æ—Å–ª–µ —Å–ª–æ–≤–∞).\n",
    "        \"\"\"\n",
    "\n",
    "        j = start + 1\n",
    "        variants = []\n",
    "        while j < len(query) + 1:   # –∏—â–µ–º –≤ —Ü–∏–∫–ª–µ —Å–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ, –ø–æ–∫–∞ –Ω–µ –∑–∞–∫–æ–Ω—á–∏—Ç—Å—è –∑–∞–ø—Ä–æ—Å\n",
    "            variant = query[start:j]\n",
    "            if variant in self.vocab:\n",
    "                variants.append([(variant, j)])\n",
    "            if len(variants) == amount: # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Ç—Ä–µ–±—É–µ–º–æ–µ –∫–æ–ª–∏—á—Å—Ç–≤–æ —Å–ª–æ–≤, –≤—ã—Ö–æ–¥–∏–º –∏–∑ —Ü–∏–∫–ª–∞ –∑–∞—Ä–∞–Ω–µ–µ\n",
    "                break\n",
    "            j += 1\n",
    "        \n",
    "        if start < len(query) and len(variants) == 0:\n",
    "            variants.append([(\"DEL\", len(query))])\n",
    "            return variants\n",
    "            \n",
    "        return variants\n",
    "    \n",
    "    def extend_variants(self, query: str, variants: List[Tuple[str, int]]) -> List[List[Tuple[str, int]]]:\n",
    "        \"\"\"\n",
    "        –§—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞—Å—à–∏—Ä—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Å–ª–æ–≤\n",
    "\n",
    "        Args:\n",
    "            query    (str)                   : –∑–∞–ø—Ä–æ—Å.\n",
    "            variants (List[List[Tuple[str, int]]]) : –°–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤ –∫–æ—Ä—Ç–µ–∂–µ–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ (—Å–ª–æ–≤–æ, –ø–æ–∑–∏—Ü–∏—è –∏–¥—É—â–∞—è –ø–æ—Å–ª–µ —Å–ª–æ–≤–∞).\n",
    "\n",
    "        Returns:\n",
    "            List[List[Tuple[str, int]]]: –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤ –∫–æ—Ä—Ç–µ–∂–µ–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ (—Å–ª–æ–≤–æ, –ø–æ–∑–∏—Ü–∏—è –∏–¥—É—â–∞—è –ø–æ—Å–ª–µ —Å–ª–æ–≤–∞).\n",
    "        \n",
    "        Example:\n",
    "            [[(—è, 1)]] -> [[('—Å', 1), ('–∫', 2)], [('—Å', 1), ('–∫–∞', 3)], [('—Å', 1), ('–∫–∞–∂–∏', 5)]]\n",
    "        \"\"\"\n",
    "\n",
    "        new_variants = []\n",
    "        for idx, variant in enumerate(variants): # –¥–æ–±–∞–≤–ª—è–µ–º –≤ –≤–∞—Ä–∏–∞–Ω—Ç—ã –≤–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è\n",
    "            for next_variant in self.get_next_variant(query, start=variant[-1][1], amount=10):\n",
    "                new_variants.append(variants[idx] + next_variant)\n",
    "        \n",
    "        return new_variants\n",
    "    \n",
    "    def get_texts(self, variants: List[List[Tuple[str, int]]]) -> List[Tuple[str, int]]:\n",
    "        \"\"\"\n",
    "        –§—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å–ø–∏—Å–æ–∫ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –≤ —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤\n",
    "\n",
    "        Args:\n",
    "            variants (List[Tuple[str, int]]) : –°–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤ –∫–æ—Ä—Ç–µ–∂–µ–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ (—Å–ª–æ–≤–æ, –ø–æ–∑–∏—Ü–∏—è –∏–¥—É—â–∞—è –ø–æ—Å–ª–µ —Å–ª–æ–≤–∞).\n",
    "\n",
    "        Returns:\n",
    "            List[Tuple[str, int]]: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ (—Ç–µ–∫—Å—Ç, –∏–Ω–¥–µ–∫—Å).\n",
    "        \n",
    "        \"\"\"\n",
    "        texts = []\n",
    "        for idx, variant in enumerate(variants): # —Ä–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã\n",
    "            text = \"\"\n",
    "            for pair in variant:\n",
    "                text += pair[0] + \" \"\n",
    "            texts.append((text.rstrip(), idx))\n",
    "        return texts\n",
    "    \n",
    "    def get_perplexity(self, text: str) -> float:\n",
    "        \"\"\"\n",
    "        –§—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è —Å—á–∏—Ç–∞–µ—Ç –ø–µ—Ä–ø–ª–µ–∫—Å–∏—é text\n",
    "\n",
    "        Args:\n",
    "            text  (str) : —Ç–µ–∫—Å—Ç.\n",
    "        Returns:\n",
    "            float: –ó–Ω–∞—á–µ–Ω–∏–µ –ø–µ—Ä–ø–ª–µ–∫—Å–∏–∏.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            encodings = self.tokenizer(text, return_tensors=\"pt\")\n",
    "            outputs = self.model(**encodings, labels=encodings.input_ids)\n",
    "            loss = outputs.loss\n",
    "            ppl = torch.exp(loss).item()\n",
    "        return ppl\n",
    "\n",
    "    def prune(self, variants: List[List[Tuple[str, int]]], beam_width: int) -> List[List[Tuple[str, int]]]:\n",
    "        \"\"\"\n",
    "        –§—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –æ—Å—Ç–∞–≤–ª—è–µ—Ç beam_width –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —Å —Å–∞–º–æ–π –Ω–∏–∑–∫–æ–π –ø–µ—Ä–ø–ª–µ–∫—Å–∏–µ–π\n",
    "\n",
    "        Args:\n",
    "            variants (List[List[Tuple[str, int]]]) : –°–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤ –∫–æ—Ä—Ç–µ–∂–µ–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ (—Å–ª–æ–≤–æ, –ø–æ–∑–∏—Ü–∏—è –∏–¥—É—â–∞—è –ø–æ—Å–ª–µ —Å–ª–æ–≤–∞).\n",
    "\n",
    "        Returns:\n",
    "            List[List[Tuple[str, int]]]: –°–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤ –∫–æ—Ä—Ç–µ–∂–µ–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ (—Å–ª–æ–≤–æ, –ø–æ–∑–∏—Ü–∏—è –∏–¥—É—â–∞—è –ø–æ—Å–ª–µ —Å–ª–æ–≤–∞).\n",
    "        \n",
    "        \"\"\"\n",
    "        pruned_variants = []\n",
    "        texts = self.get_texts(variants)\n",
    "        indices = [x[1] for x in sorted(texts, key=lambda x: self.get_perplexity(x[0]) / (5 * x[1] + 1) )[:beam_width]]\n",
    "        for i in indices:\n",
    "            pruned_variants.append(variants[i])\n",
    "        return pruned_variants\n",
    "    \n",
    "    def sieve(self, variants):\n",
    "        \"\"\"\n",
    "        –§—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –æ—Ç—Å–µ–∏–≤–∞–µ—Ç —Ç—É–ø–∏–∫–æ–≤—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã\n",
    "\n",
    "        Args:\n",
    "            variants (List[List[Tuple[str, int]]]) : –°–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤ –∫–æ—Ä—Ç–µ–∂–µ–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ (—Å–ª–æ–≤–æ, –ø–æ–∑–∏—Ü–∏—è –∏–¥—É—â–∞—è –ø–æ—Å–ª–µ —Å–ª–æ–≤–∞).\n",
    "\n",
    "        Returns:\n",
    "            List[List[Tuple[str, int]]]: –ü—Ä–æ—Å–µ–µ–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        sieved = []\n",
    "        for variant in variants: # –Ω–∞—à–ª–∏ –º–µ—Ç–∫—É \"DEL\" -> —É–¥–∞–ª—è–µ–º –≤–∞—Ä–∏–∞–Ω—Ç\n",
    "            if variant[-1][0] != \"DEL\":\n",
    "                sieved.append(variant)\n",
    "        return sieved\n",
    "    \n",
    "    def decode(self, result: List[List[Tuple[str, int]]]) -> Tuple[List[int], str]:\n",
    "        \"\"\"\n",
    "        –§—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤–∞—Ä–∏–∞–Ω—Ç –≤ —Å–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤, –≥–¥–µ –¥–æ–ª–∂–Ω—ã —Å—Ç–æ—è—Ç—å –ø—Ä–æ–±—É–ª—ã –∏ –ø–æ–ª—É—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç\n",
    "\n",
    "        Args:\n",
    "            result (List[List[Tuple[str, int]]]) : –°–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤ –∫–æ—Ä—Ç–µ–∂–µ–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ (—Å–ª–æ–≤–æ, –ø–æ–∑–∏—Ü–∏—è –∏–¥—É—â–∞—è –ø–æ—Å–ª–µ —Å–ª–æ–≤–∞).\n",
    "\n",
    "        Returns:\n",
    "            Tuple[\n",
    "                List[int] : —Å–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤, –≥–¥–µ –¥–æ–ª–∂–Ω—ã —Å—Ç–æ—è—Ç—å –ø—Ä–æ–±–µ–ª—ã\n",
    "                str       : –∑–∞–ø—Ä–æ—Å —Å —Ä–∞—Å—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –ø—Ä–æ–±–µ–ª–∞–º–∏\n",
    "            ]\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        indices = [x[1] for x in result]\n",
    "        text = [x[0] for x in result]\n",
    "        return indices, \" \".join(text).rstrip()\n",
    "    \n",
    "    def query_preprocessor(self, query : str) -> Tuple[List[str], List[Tuple[int, int]], List[str]]:\n",
    "        \"\"\"\n",
    "        –§—É–Ω–∫—Ü–∏—è, –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–∞—è –∑–∞ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É –∑–∞–ø—Ä–æ—Å–∞.\n",
    "        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –Ω–∞–ª–∏—á–∏–µ —Å–ø–µ—Ü. —Å–∏–º–≤–æ–ª–æ–≤ –∏—Ö –ø–æ–ª–æ–∂–µ–Ω–∏–µ –∏ —Ç–∏–ø.\n",
    "        –ï—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ - –∑–∞–ø—Ä–æ—Å —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞ –ø–æ–¥–∑–∞–ø—Ä–æ—Å—ã\n",
    "\n",
    "        Args:\n",
    "            query (str): –∑–∞–ø—Ä–æ—Å.\n",
    "\n",
    "        Returns:\n",
    "        Tuple[\n",
    "            List[str]            : –ø–æ–¥–∑–∞–ø—Ä–æ—Å—ã\n",
    "            List[Tuple[int, int] : —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ —Å–ø–µ—Ü. —Å–∏–º–≤–æ–ª–æ–≤\n",
    "            List[str]]           : —Ç–∏–ø—ã –≥—Ä—É–ø–ø —Å–ø–µ—Ü. —Å–∏–º–≤–æ–ª–æ–≤\n",
    "        ]\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        query = query.lower()\n",
    "        idxs = []\n",
    "        types = []\n",
    "        \n",
    "        for i in range(len(query)): # –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –ø–æ –≥—Ä—É–ø–ø–∞–º\n",
    "            if query[i] in self.digits:\n",
    "                types.append('d')\n",
    "                idxs.append(i)\n",
    "            if query[i] in self.punkt:\n",
    "                types.append('p')\n",
    "                idxs.append(i)\n",
    "            if query[i] in self.letters:\n",
    "                types.append('l')\n",
    "                idxs.append(i)\n",
    "\n",
    "\n",
    "        if idxs:\n",
    "            compressed_types = [types[0]] \n",
    "            start = end = idxs[0]\n",
    "            indices = []\n",
    "            for i in range(1, len(idxs)): # –æ—é—ä–µ–¥–∏–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –æ—Ç—Ä–µ–∑–∫–∏\n",
    "                if idxs[i] == idxs[i-1] + 1 and types[i] == types[i-1]:\n",
    "                    end = idxs[i]\n",
    "                else:\n",
    "                    indices += [(start, end)] if start != end else [(start, start)]\n",
    "                    start = end = idxs[i]\n",
    "            \n",
    "            indices += [(start, end)] if start != end else [(start, start)]\n",
    "\n",
    "            for i in range(1, len(types)): # –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–∏–ø—ã –≤ –≥—Ä—É–ø–ø—ã\n",
    "                if types[i] != compressed_types[-1]:\n",
    "                    compressed_types.append(types[i])\n",
    "\n",
    "            qs = []\n",
    "            first = query[:indices[0][0]]\n",
    "            if first != \"\":\n",
    "                qs.append(first)\n",
    "            for i in range(1, len(indices)): # —Ä–∞–∑–±–∏–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã –Ω–∞ –ø–æ–¥–∑–∞–ø—Ä–æ—Å—ã\n",
    "                sub_query = query[indices[i-1][1] + 1:indices[i][0]]\n",
    "                if sub_query != \"\":\n",
    "                    qs.append(sub_query)\n",
    "            last = query[indices[-1][1] + 1:]\n",
    "            if last != \"\":\n",
    "                qs.append(last)\n",
    "            return qs, indices, compressed_types\n",
    "        \n",
    "        return [query], [], []\n",
    "\n",
    "    def query_processor(self, query: str, beam_width: int=5, is_subquery=False) -> List[List[Tuple[str, int]]]:\n",
    "\n",
    "        \"\"\"\n",
    "        –§—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ —Å–ª–æ–≤ —Å –ø–æ–º–æ—â—å—é Beam-Search\n",
    "\n",
    "        Args:\n",
    "            query      (str)  : –∑–∞–ø—Ä–æ—Å.\n",
    "            beam_width (int)  : –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –æ—Å—Ç–∞–Ω—É—Ç—å—Å—è\n",
    "            is_subquery (bool): —Ñ–ª–∞–≥, –æ–±–æ–∑–Ω–∞—á–∞—é—â–∏–π, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø—Ä–æ—Å –ø–æ–¥–∑–∞–ø—Ä–æ—Å–æ–º\n",
    "\n",
    "        Returns:\n",
    "            List[List[Tuple[str, int]]]: –°–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤ –∫–æ—Ä—Ç–µ–∂–µ–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ (—Å–ª–æ–≤–æ, –ø–æ–∑–∏—Ü–∏—è –∏–¥—É—â–∞—è –ø–æ—Å–ª–µ —Å–ª–æ–≤–∞)\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        if is_subquery == True and query in self.vocab: # –ï—Å–ª–∏ –ø—Ä–∏–ª–µ—Ç–∞–µ—Ç –ø–æ–¥–∑–∞–ø—Ä–æ—Å, –ø—ã—Ç–∞–µ–º—Å—è –∑–∞–∫—Ä—ã—Ç—å –µ–≥–æ –ø–æ–¥–≥–ª—è–¥—ã–≤–∞–Ω–∏–µ–º –≤ —Å–ª–æ–≤–∞—Ä—å\n",
    "            return [[(query, len(query))]]\n",
    "        \n",
    "        variants = self.get_next_variant(query, start=0, amount=5) # –ø–æ–ª—É—á–µ–Ω–∏–µ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤\n",
    "        saved = []\n",
    "        i = 0\n",
    "\n",
    "        while i < 20 and saved == []: # –∞–ª—è Beam-Search –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —Å –Ω–∞–∏–º–µ–Ω—å—à–µ–π –ø–µ—Ä–ø–ª–µ–∫—Å–∏–µ–π\n",
    "\n",
    "            variants = self.extend_variants(query, variants)\n",
    "\n",
    "            variants = self.sieve(variants)\n",
    "            for variant in variants:\n",
    "                if variant[-1][1] == len(query): # –ï—Å–ª–∏ –º—ã –¥–æ—à–ª–∏ –¥–æ –∫–æ–Ω—Ü–∞ –∑–∞–ø—Ä–æ—Å–∞, –ø–µ—Ä–µ–¥ –Ω–∞–º–∏ –æ—Ç–≤–µ—Ç\n",
    "                    saved.append(variant)\n",
    "                    self.result = saved[0]\n",
    "                    return saved\n",
    "            variants = self.prune(variants, beam_width)\n",
    "            \n",
    "            i += 1\n",
    "        self.result = saved[0]\n",
    "        return saved\n",
    "    \n",
    "    def merge_subresults(self, subresults: List[List[List[Tuple[str, int]]]], indices: List[Tuple[int, int]] , types: List[str], query: str) -> List[int]:\n",
    "        \"\"\"\n",
    "        –§—É–Ω–∫—Ü–∏—è, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è –æ—Ç–≤–µ—Ç—ã –Ω–∞ –ø–æ–¥–∑–∞–ø—Ä–æ—Å—ã –≤ –µ–¥–∏–Ω—ã–π –æ—Ç–≤–µ—Ç\n",
    "\n",
    "        Args:\n",
    "            subresults (List[List[List[Tuple[str, int]]]]) : –°–ø–∏—Å–æ–∫ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –ø–æ–¥–∑–∞–ø—Ä–æ—Å—ã\n",
    "            indices (List[Tuple[int, int]])                : —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ —Å–ø–µ—Ü. —Å–∏–º–≤–æ–ª–æ–≤\n",
    "            types (List[str])                              : —Ç–∏–ø—ã –≥—Ä—É–ø–ø —Å–ø–µ—Ü. —Å–∏–º–≤–æ–ª–æ–≤\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            List[int] - –°–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤, –Ω–∞ –º–µ—Å—Ç–µ –∫–æ—Ç–æ—Ä—ã—Ö –¥–æ–ª–∂–µ–Ω —Å—Ç–æ—è—Ç—å –ø—Ä–æ–±–µ–ª\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        i, j = 0, 0 \n",
    "        global_ptr = 0\n",
    "        rel_idxs = []\n",
    "        result = []\n",
    "\n",
    "        # –ø–æ–ª—É—á–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏ –ø—Ä–æ–±–µ–ª–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–¥–∑–∞–ø—Ä–æ—Å–∞\n",
    "        for subresult in subresults:\n",
    "            idxs, _ = self.decode(subresult[0])\n",
    "            rel_idxs.append(idxs)\n",
    "        \n",
    "        # –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–µ—Ü. —Å–∏–º–≤–æ–ª–æ–≤ –∏ –ø–æ–¥–∑–∞–ø—Ä–æ—Å–æ–≤\n",
    "        while i < len(indices) and j < len(rel_idxs):\n",
    "\n",
    "            rel_ptr = rel_idxs[j][0] if rel_idxs[j] else 0\n",
    "            \n",
    "            if indices[i][0] < global_ptr + rel_ptr: # –ø–µ—Ä–µ–¥ –Ω–∞–º–∏ —Å–ø–µ—Ü. —Å–∏–º–≤–æ–ª\n",
    "                \n",
    "                if indices[i][0] != 0 and (not result or result[-1] != indices[i][0]):\n",
    "                    result.append(indices[i][0])  # –ø—Ä–∞–≤—ã–π –ø—Ä–æ–±–µ–ª\n",
    "                if indices[i][1] != len(query) and (not result or result[-1] != indices[i][1] + 1):\n",
    "                    result.append(indices[i][1] + 1)  # –ª–µ–≤—ã–π –ø—Ä–æ–±–µ–ª\n",
    "                \n",
    "                global_ptr = indices[i][1] + 1 \n",
    "                i += 1\n",
    "            else:\n",
    "                # –ø–µ—Ä–µ–¥ –Ω–∞–º–∏ –ø–æ–¥–∑–∞–ø—Ä–æ—Å\n",
    "                for rel_idx in rel_idxs[j]:\n",
    "                    abs_pos = rel_idx + global_ptr\n",
    "                    if not result or result[-1] != abs_pos:\n",
    "                        result.append(abs_pos)\n",
    "                \n",
    "                if rel_idxs[j]:\n",
    "                    global_ptr += rel_idxs[j][-1]\n",
    "                j += 1\n",
    "\n",
    "        # –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è —Å–ø–µ—Ü. —Å–∏–º–≤–æ–ª–æ–≤\n",
    "        \n",
    "        while i < len(indices):\n",
    "            if indices[i][0] != 0 and (not result or result[-1] != indices[i][0]) and types[i] != 'p':\n",
    "                result.append(indices[i][0])\n",
    "            if indices[i][1] != len(query) and (not result or result[-1] != indices[i][1] + 1):\n",
    "                result.append(indices[i][1] + 1)\n",
    "            \n",
    "            global_ptr = indices[i][1] + 1\n",
    "            i += 1\n",
    "\n",
    "        # –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –ø–æ–¥–∑–∞–ø—Ä–æ—Å–æ–≤\n",
    "        while j < len(rel_idxs):\n",
    "            for rel_idx in rel_idxs[j]:\n",
    "                abs_pos = rel_idx + global_ptr\n",
    "                if not result or result[-1] != abs_pos:\n",
    "                    result.append(abs_pos)\n",
    "            \n",
    "            if rel_idxs[j]:\n",
    "                global_ptr += rel_idxs[j][-1]\n",
    "            j += 1\n",
    "\n",
    "        # —É–¥–∞–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø—Ä–æ–±–µ–ª, –µ—Å–ª–∏ –æ–Ω –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ –≥—Ä–∞–Ω–∏—Ü—ã –∑–∞–ø—Ä–æ—Å–∞\n",
    "        if result and result[-1] == len(query):\n",
    "            result.pop()\n",
    "        \n",
    "        # –∑–Ω–∞–∫–∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ —Å—ä–µ–¥–∞—é—Ç –ø—Ä–æ–±–µ–ª —Å–ª–µ–≤–∞ –æ—Ç —Å–µ–±—è\n",
    "        for i in range(len(query)):\n",
    "            if query[i] in self.punkt:\n",
    "                result.remove(i)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def impute_whitespaces(self, query: str) -> List[int]:\n",
    "        \"\"\"\n",
    "        –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–æ—Å—Ç–æ –∑–∞–ø—É—Å–∫–∞–µ—Ç –≤–µ—Å—å –ø–∞–π–ø–ª–∞–π–Ω.\n",
    "\n",
    "        Args:\n",
    "            query (str): –∑–∞–ø—Ä–æ—Å.\n",
    "\n",
    "        Returns:\n",
    "            List[int] : —Å–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤, –≥–¥–µ –¥–æ–ª–∂–Ω—ã —Å—Ç–æ—è—Ç—å –ø—Ä–æ–±–µ–ª—ã\n",
    "        \n",
    "        \"\"\"\n",
    "        try:\n",
    "            # –ø—Ä–æ–±—É–µ–º –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∑–∞–ø—Ä–æ—Å\n",
    "            query = query.lower()\n",
    "            subresults = []\n",
    "            subqueries, indices, types = self.query_preprocessor(query) # –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞\n",
    "            for subquery in subqueries:\n",
    "                subresults.append(self.query_processor(subquery, is_subquery=len(subqueries)>1 or indices != [])) # –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞\n",
    "            \n",
    "            \n",
    "            if indices == []: # –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–µ –±—ã–ª–æ, —Ç–æ –ø—Ä–æ—Å—Ç–æ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∫ –æ—Ç–≤–µ—Ç—É\n",
    "                return self.decode(subresults[0][0])[0]\n",
    "            \n",
    "            result = self.merge_subresults(subresults, indices, types, query) # –µ—Å–ª–∏ –±—ã–ª–∏ –ø–æ–¥–∑–∞–ø—Ä–æ—Å—ã - –æ–±—ä–µ–¥–∏–Ω—è–µ–º –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º\n",
    "            return result\n",
    "        except: # –µ—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∑–∞–ø—Ä–æ—Å, –≤—ã–¥–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –∏ –≤—ã–≤–æ–¥–∏–º –∑–∞–ø—Ä–æ—Å, –≤—ã–∑–≤–∞–≤–≤—à–∏–π –æ—à–∏–±–∫—É \n",
    "            print(query)\n",
    "            return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8e18657b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = PerplexityImputer(\n",
    "    vocab,\n",
    "    model,\n",
    "    tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "846b4ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(data_df, imputer, path: str = \"submission.csv\") -> None:\n",
    "    data_df[\"predicted_positions\"] = data_df.apply(lambda row: imputer.impute_whitespaces(row[\"text_no_spaces\"]), axis=1)\n",
    "    submission = data_df.drop(columns=[\"text_no_spaces\"])\n",
    "    submission[\"predicted_positions\"] = submission[\"predicted_positions\"].astype(str)\n",
    "    submission.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04e516f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "—è–∫–∞–ª–µ–Ω–¥–∞—Ä—å–ø–µ—Ä–µ–≤–µ—Ä–Ω—É–ª\n",
      "—Ç–≤–æ—è–¥–æ—á—å–∏–º–æ–π—Å—ã–Ω‚Äî\n",
      "–∞—Ä–∫–∞–¥–∏–π—É–∫—É–ø–Ω–∏–∫‚Äî–∫—É—Å–∞—é—â–∏–π–ª–∂–µ—Ü\n",
      "–º—ã–ø—å–µ–º—á–∞–π–≤—Å—Ç–∞—Ä—ã—Ö–∫–≤–∞—Ä—Ç–∏—Ä–∞—Ö,\n",
      "–æ–¥–Ω–∞—Å–µ–º—å—è,–¥–≤–µ—Å–µ–º—å–∏,—Ç—Ä–∏—Å–µ–º—å–∏...\n",
      "—Ä—è–¥–æ–º—Å–º–µ—Ç—Ä–æ,—Ü–µ–Ω—Ç—Ä...\n",
      "–≤—Å–µ–≥–æ–≤–æ—Ä—è—Ç,—á—Ç–æ–º—ã–≤-–º–µ—Å—Ç–µ...\n",
      "—Å—Ç–æ–π!–æ–ø–∞—Å–Ω–∞—è–∑–æ–Ω–∞!—Ä–∞–±–æ—Ç–∞–º–æ–∑–≥–∞!..\n",
      "...–≥–æ–¥–∞–º–∏–¥–æ–ª–≥–∏–º–∏.\n",
      "...–Ω–æ—á–∞–º–∏—Ç–µ–º–Ω—ã–º–∏.\n",
      "–∏—Å–∫–∞–ª–∞—Ç–µ–±—è–Ω–æ—á–∞–º–∏,—á–∞–º–∏,—á–∞–º–∏,—á–∞–º–∏...\n",
      "–∞–±–µ–∑–º—É–∑—ã–∫–∏–Ω–∞–º–∏—Ä—É—Å–º–µ—Ä—Ç—å–Ω–µ–∫—Ä–∞—Å–Ω–∞,\n",
      "—Å—á–∞—Å—Ç—å–µ–¥–∞–Ω–æ–ø–æ–≤—Å—Ç—Ä–µ—á–∞—Ç—å–∏–ª—å–±–µ–¥—É–µ—â–µ\n",
      "–ø—Ä–∏–∫–∞–∑–∞–≤–µ—Ä–∏—Ç—å–≤—á—É–¥–µ—Å–∞\n",
      "–∫–æ–ª–æ—Ç—å—É—Å—Ç–∞–ª–∞.\n",
      "–≤–æ–ª–∫–∏—É—Ö–æ–¥—è—Ç...\n",
      "–±–∏–ª–µ—Ç–≤–æ–¥–∏–Ω–∫–æ–Ω–µ—Ü‚Äî\n",
      "—è–ª—é–±–ª—é—Å–ª—É—à–∞—Ç—å–ø–µ—Å–Ω–∏,–∏–∫–æ—Å—Ç—Ä–∞–Ω—é—Ö–∞—Ç—å–¥—ã–º\n",
      "—Å–µ—Ä–¥—Ü–µ–±—å–µ—Ç—Å—è–¥—Ä—É–≥–∏–º–∏–≤–µ—Ä—à–∏–Ω–∞–º–∏\n",
      "—Å–∏–≥–∞—Ä–µ—Ç–∫–∞–æ—Å—Ç–∞–ª–∞—Å—å–≤—Å–µ–≥–æ–æ–¥–Ω–∞,–Ω—É–ª–∞–¥–Ω–æ–Ω–∞...\n",
      "–∑–∞–º—ã–∫–∞–Ω–∏–µ+–±–∞—Ö!–∏–≤—Å—Ç–∞–ª–∏—Ç—Ä–∞–º–≤–∞–∏\n",
      "–æ–Ω–∞—Å–∫–∞–∂–µ—Ç:'–∞,–≤–æ–æ–±—â–µ,–≤–æ–ª–æ–¥—å–∫–∞,—Ö—Ä–µ–Ω–æ–≤–æ!'\n",
      "–ø—Ä–∏—à—ë–ª–∫–∫–æ–ª–¥—É–Ω—å–µ:'–∞–Ω—É-–∫–∞–Ω–∞–∫–æ–ª–¥—É–π–º–Ω–µ!'\n",
      "'–ª–µ–≥–∫–æ,–º–æ–π—Ö–æ—Ä–æ—à–∏–π,—Ç–æ–ª—å–∫–æ—Ö–ª–æ–ø–Ω—É–≤–ª–∞–¥–æ—à–∏,\n",
      "–∏–º–∞–ª–µ–Ω—å–∫–∞—è–∂–∏–∑–Ω—å–≤–Ω—É—Ç—Ä–∏–Ω–µ—ë–æ–±–æ—Ä–≤—ë—Ç—Å—è.'\n",
      "–ø—Ä–∏–≤–∏–¥–µ–Ω–∏–π,–≥–æ–≤–æ—Ä—è—Ç:'–Ω–µ–±—ã–≤–∞—Ç—å–ø—Ä–µ—Å—Ç—É–ø–ª–µ–Ω—å—é'\n",
      "—Å–¥–∞–≤–∞–π—Å—è,–≤–µ–¥—å–º–∞‚Äì¬´–Ω–æ—á–Ω–æ–π–¥–æ–∑–æ—Ä¬ª.\n",
      "–ø–æ—Ç—ë–º–Ω—ã–º—É–ª–∏—Ü–∞–º–ª–µ—Ç–∏—Ç¬´–Ω–æ—á–Ω–æ–π–¥–æ–∑–æ—Ä¬ª.\n",
      "–Ω–æ—Å–∏–ª–∞¬´–∏–Ω–æ–≥–æ¬ª–≤–∞–Ω—Ç–æ–Ω–æ–≤–æ–º–≤–∑–æ—Ä–µ,\n",
      "–∞–∑–Ω–∞—á–∏—Ç,–æ–Ω–±—É–¥–µ—Ç—Ä–∞–±–æ—Ç–∞—Ç—å–≤¬´–¥–æ–∑–æ—Ä–µ¬ª.\n",
      "–µ—â–µ–±—ã,–∞–±—ã–∫–æ–≥–æ–Ω–µ–±–µ—Ä—É—Ç–≤–∑–∞–º–º–∏–Ω–∏—Å—Ç—Ä—ã.\n",
      "–Ω–æ–∑–ª–æ–π–∑–∞–≤—É–ª–æ–Ω–∏–∑¬´–¥–æ–∑–æ—Ä–∞–¥–Ω–µ–≤–Ω–æ–≥–æ¬ª\n",
      "—Å–¥–µ–ª–∞–ª–∏–∑—Å—ã–Ω–∞–∞–Ω—Ç–æ–Ω–∞¬´–∏–Ω–æ–≥–æ¬ª.\n",
      "–∏–Ω–æ–≤–æ–µ—É—Ç—Ä–æ,–∏–Ω–æ–≤–æ–µ'–Ω–µ—Ç—Ç–µ–±—è...'\n",
      "'–¥—Ä—É–∑—å—è—Ö–æ—Ç—è—Ç–ø–æ–∫—É—à–∞—Ç—å,–ø–æ–π–¥—ë–º,–ø—Ä–∏—è—Ç–µ–ª—å,–≤–ª–µ—Å!'\n",
      "—Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã–π–∑–≤–æ–Ω–æ–∫,–∫–∞–∫–∫–æ–º–∞–Ω–¥–∞'–≤–ø–µ—Ä–µ–¥!'\n",
      "–∞–µ—â–µ–∫—Ä–∞—Å–∏–≤—ã–π–≥–∞–ª—Å—Ç—É–∫—É–º–µ–Ω—è.\n"
     ]
    }
   ],
   "source": [
    "create_submission(data_df, pi, \"submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4da2dd",
   "metadata": {},
   "source": [
    "–ê–ª–≥–æ—Ä–∏—Ç–º –æ—Ç–∫–∞–∑–∞–ª—Å—è —Ä–∞–∑–º–µ—á–∞—Ç—å 37 —Å—ç–º–ø–ª–æ–≤ –∏–∑ 1004, –æ –ø—Ä–∏—á–∏–Ω–∞—Ö –ø–æ–¥—É–º–∞–µ–º –ø–æ–∑–¥–Ω–µ–µ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937c2ef0",
   "metadata": {},
   "source": [
    "–ó–∞—Å—ã–ª–∞–µ–º —Å–∞–±–º–∏—Ç, –ø–æ–ª—É—á–µ–º —Å–∫–æ—Ä: Your Mean F1 = **79.266%** üéâüéâüéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15778d62",
   "metadata": {},
   "source": [
    "## –ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c03d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_imputer_max(query : str, vocab, vc=10, bl=10) -> list[int]:\n",
    "    # –§-—Ü–∏—è —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ —á–∞—Å—Ç—å –ø—Ä–æ—Å—Ç—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ —è–≤–ª—è–µ—Ç—Å—è –ø—Ä–æ—Å—Ç–æ proof-of-concept!\n",
    "    try:\n",
    "        print(query)\n",
    "        variants = []\n",
    "        whitespaces = []\n",
    "        buffer = \"\"\n",
    "        result = []\n",
    "        parsed_index = 0\n",
    "        \n",
    "        query = query.lower()\n",
    "        i = 0\n",
    "        while i < len(query):\n",
    "            buffer += query[i]\n",
    "            \n",
    "            if buffer in vocab:\n",
    "                print(buffer)\n",
    "                variants.append(buffer)\n",
    "\n",
    "            if len(variants) == vc or len(buffer) == bl or i + 1 == len(query):\n",
    "                print(f\"\\n\\nbuffer overflow : {len(buffer) == bl}\\nvariant overflow : {len(variants) == vc}\\n\\n\")\n",
    "                print(variants)\n",
    "                result.append(max(list(zip(map(len, variants), variants)), key=lambda x: x[0])[1])\n",
    "                parsed_index += len(result[-1])\n",
    "                whitespaces.append(parsed_index)\n",
    "                buffer = \"\"\n",
    "                variants = []\n",
    "                i = parsed_index\n",
    "                print(f\"{result[-1]} : saved\")\n",
    "                continue\n",
    "\n",
    "\n",
    "            i += 1\n",
    "    except:\n",
    "        return \" \""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
