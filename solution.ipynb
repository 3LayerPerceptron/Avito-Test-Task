{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "853dbcc9",
   "metadata": {},
   "source": [
    "## Рассуждения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda3d3e5",
   "metadata": {},
   "source": [
    "### Идея 1: взять токенайзер и побить каждый из запросов на токены.\n",
    "\n",
    "Эта идея сразу отпала, потому что большая часть токенайзеров предполагает, что слова в корпусе будут разделены пробелом.\n",
    "\n",
    "В добавок, ну, вот побили мы на токены, а как объединять эти токены в слова? (чтобы понять где должны стоять пробелы)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c62991",
   "metadata": {},
   "source": [
    "### Идея 2: Накопление Буфера\n",
    "\n",
    "Давайте отталкиваться от недостатков первой идеи, а именно как объединять эти токены в слова?\n",
    "\n",
    "Сделаем буфер, куда будем добавлять каждый символ из запроса. Как только мы соберем несколько кандидатов на слова, выберем из них наиболее длинное и поставим после него пробел.\n",
    "\n",
    "Эта идея подразумевает наличие корпуса, в который мы будем подглядывать, чтобы понять является набор символов словом или нет.\n",
    "\n",
    "Поэтому я смотался на kaggle и взял OpenCorpora-russan https://www.kaggle.com/datasets/rtatman/opencorpora-russian\n",
    "\n",
    "Реализацию 2-й идеи можно найти в разделе артефакты **cumulative_imputer_max**.\n",
    "\n",
    "P.S. Почему выбирается максимальное по длине слово?\n",
    "\n",
    "Это эвристика, основанная на том, что мы хотим полностью разбить запрос на слова. Т.е. нам в каком-то смысле выгодно откусыввать большие куски запроса.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894ecfc8",
   "metadata": {},
   "source": [
    "### Докручиваем идею 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6141bcb7",
   "metadata": {},
   "source": [
    "**Успешные** примеры\n",
    "\n",
    "отдамдаромкошку -> отдам даром кошку\n",
    "\n",
    "новыйдивандоставканедорого -> новый диван доставка недорого\n",
    "\n",
    "**Неудачные** примеры\n",
    "\n",
    "работавмосквеудаленно -> работав москве удал е н но\n",
    "\n",
    "сдаюквартирусмебельюитехникой -> сдаю квартиру см е белью ит е х никой"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9431f5ff",
   "metadata": {},
   "source": [
    "Проблема с неудачными примерами очевидна. Алгоритм работает по эвристике, которая никогда не выберет **работа**, если можно выбрать деепричастие **работав** или **с**, если можно выбрать **см**.\n",
    "\n",
    "Если мы хотим выбирать комбинации слов, которые наиболее похожи на то, что ввел человек, то нам не избежать использование частот/статистики употребеления того или иного слова.\n",
    "\n",
    "Вряд ли удастся найти такой список, который полность совпал бы с нашим словарем, поэтому давайте возьмем маленькую языковую модель и будем считать перплексию, вместо максимальной длины слова.\n",
    "\n",
    "У странных слов по типу **в москве удал е** пералексия будет больше, чем у **в москве удаленно**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3ce388",
   "metadata": {},
   "source": [
    "Модельку взял с hf https://huggingface.co/kplro/rugpt3small_based_on_gpt2-l2_russian_10_epoch какой-то небольшой чекпоинт, который как раз подходит под наши цели\n",
    "\n",
    "P.S. Да в идеале обучить что-то свое и поменьше на корпусе объявлений, но как proof-of-concept нам подойдет и эта."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5efee3",
   "metadata": {},
   "source": [
    "### В разделе код представлена реализации докрученной второй идеи (Конкретно в классе Perplexity imputer)\n",
    "Большая часть кода - чисто техническая перегонка из одного формата в другой и попытка в обработку спец. символов (пунктуация, цифры, знаки препинания и т.д.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f44caf2",
   "metadata": {},
   "source": [
    "## Код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7c2365",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install transformers\n",
    "!pip3 install pandas\n",
    "!pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ca1540",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip archive.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27d333f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418b7111",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"dataset_1937770_3.txt\"\n",
    "VOCAB_PATH = \"dictionary.txt\"\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80109e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(data_path, header=True):\n",
    "    data = []\n",
    "    col_names = []\n",
    "    with open(data_path, 'r') as file:\n",
    "        i = 0\n",
    "        for line in file:\n",
    "            if header:\n",
    "                col_names = line.strip().split(\",\")\n",
    "                header = False\n",
    "                continue\n",
    "            data.append([i, \",\".join(line.split(\",\")[1:]).strip()])\n",
    "            i += 1\n",
    "    return pd.DataFrame(data, columns=col_names)\n",
    "\n",
    "def vocab_loader(vocab_path):\n",
    "    vocab = set()\n",
    "    with open(vocab_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.strip().isnumeric():\n",
    "                continue\n",
    "            vocab.add(line.split(\"\\t\")[0].strip().lower())\n",
    "    vocab.add(\"бу\")\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25fce5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vocab_loader(VOCAB_PATH)\n",
    "data_df = data_loader(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82c24479",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"kplro/rugpt3small_based_on_gpt2-l2_russian_10_epoch\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"kplro/rugpt3small_based_on_gpt2-l2_russian_10_epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dcca05d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerplexityImputer():\n",
    "    \n",
    "    def __init__(self, vocab, model, tokenizer):\n",
    "        # parameters\n",
    "        \n",
    "        # storage\n",
    "        self.vocab = vocab\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.letters = set(\"abcdefghijklmnopqrstuvwxyz-–’\")\n",
    "        self.digits = set(\"0123456789\")\n",
    "        self.punkt = set(\".,;!?:…\")\n",
    "        return\n",
    "    \n",
    "    def get_next_variant(self, query: str, start: int = 0, amount: int = 1) -> List[List[Tuple[str, int]]]:\n",
    "        \"\"\"\n",
    "        Функция, которая выдает amount возможных слов начинающихся с позиции start\n",
    "\n",
    "        Args:\n",
    "            query  (str) : запрос.\n",
    "            start  (int) : позиция, с которой надо начинать искать слова.\n",
    "            amount (int) : максимальное количество слов, которое нужно найти.\n",
    "\n",
    "        Returns:\n",
    "            List[List[Tuple[str, int]]]: Список списков кортежей в формате (слово, позиция идущая после слова).\n",
    "        \"\"\"\n",
    "\n",
    "        j = start + 1\n",
    "        variants = []\n",
    "        while j < len(query) + 1:   # ищем в цикле следующее слово, пока не закончится запрос\n",
    "            variant = query[start:j]\n",
    "            if variant in self.vocab:\n",
    "                variants.append([(variant, j)])\n",
    "            if len(variants) == amount: # Если нашли требуемое количство слов, выходим из цикла заранее\n",
    "                break\n",
    "            j += 1\n",
    "        \n",
    "        if start < len(query) and len(variants) == 0:\n",
    "            variants.append([(\"DEL\", len(query))])\n",
    "            return variants\n",
    "            \n",
    "        return variants\n",
    "    \n",
    "    def extend_variants(self, query: str, variants: List[Tuple[str, int]]) -> List[List[Tuple[str, int]]]:\n",
    "        \"\"\"\n",
    "        Функция, которая расширяет возможные комбинации слов\n",
    "\n",
    "        Args:\n",
    "            query    (str)                   : запрос.\n",
    "            variants (List[List[Tuple[str, int]]]) : Список списков кортежей в формате (слово, позиция идущая после слова).\n",
    "\n",
    "        Returns:\n",
    "            List[List[Tuple[str, int]]]: Расширенный список списков кортежей в формате (слово, позиция идущая после слова).\n",
    "        \n",
    "        Example:\n",
    "            [[(я, 1)]] -> [[('с', 1), ('к', 2)], [('с', 1), ('ка', 3)], [('с', 1), ('кажи', 5)]]\n",
    "        \"\"\"\n",
    "\n",
    "        new_variants = []\n",
    "        for idx, variant in enumerate(variants): # добавляем в варианты возможные продолжения\n",
    "            for next_variant in self.get_next_variant(query, start=variant[-1][1], amount=10):\n",
    "                new_variants.append(variants[idx] + next_variant)\n",
    "        \n",
    "        return new_variants\n",
    "    \n",
    "    def get_texts(self, variants: List[List[Tuple[str, int]]]) -> List[Tuple[str, int]]:\n",
    "        \"\"\"\n",
    "        Функция, которая преобразует список вариантов в список текстов\n",
    "\n",
    "        Args:\n",
    "            variants (List[Tuple[str, int]]) : Список списков кортежей в формате (слово, позиция идущая после слова).\n",
    "\n",
    "        Returns:\n",
    "            List[Tuple[str, int]]: Список кортежей в формате (текст, индекс).\n",
    "        \n",
    "        \"\"\"\n",
    "        texts = []\n",
    "        for idx, variant in enumerate(variants): # распаковываем варианты\n",
    "            text = \"\"\n",
    "            for pair in variant:\n",
    "                text += pair[0] + \" \"\n",
    "            texts.append((text.rstrip(), idx))\n",
    "        return texts\n",
    "    \n",
    "    def get_perplexity(self, text: str) -> float:\n",
    "        \"\"\"\n",
    "        Функция, которая считает перплексию text\n",
    "\n",
    "        Args:\n",
    "            text  (str) : текст.\n",
    "        Returns:\n",
    "            float: Значение перплексии.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            encodings = self.tokenizer(text, return_tensors=\"pt\")\n",
    "            outputs = self.model(**encodings, labels=encodings.input_ids)\n",
    "            loss = outputs.loss\n",
    "            ppl = torch.exp(loss).item()\n",
    "        return ppl\n",
    "\n",
    "    def prune(self, variants: List[List[Tuple[str, int]]], beam_width: int) -> List[List[Tuple[str, int]]]:\n",
    "        \"\"\"\n",
    "        Функция, которая оставляет beam_width вариантов с самой низкой перплексией\n",
    "\n",
    "        Args:\n",
    "            variants (List[List[Tuple[str, int]]]) : Список списков кортежей в формате (слово, позиция идущая после слова).\n",
    "\n",
    "        Returns:\n",
    "            List[List[Tuple[str, int]]]: Список списков кортежей в формате (слово, позиция идущая после слова).\n",
    "        \n",
    "        \"\"\"\n",
    "        pruned_variants = []\n",
    "        texts = self.get_texts(variants)\n",
    "        indices = [x[1] for x in sorted(texts, key=lambda x: self.get_perplexity(x[0]) / (5 * x[1] + 1) )[:beam_width]]\n",
    "        for i in indices:\n",
    "            pruned_variants.append(variants[i])\n",
    "        return pruned_variants\n",
    "    \n",
    "    def sieve(self, variants):\n",
    "        \"\"\"\n",
    "        Функция, которая отсеивает тупиковые варианты\n",
    "\n",
    "        Args:\n",
    "            variants (List[List[Tuple[str, int]]]) : Список списков кортежей в формате (слово, позиция идущая после слова).\n",
    "\n",
    "        Returns:\n",
    "            List[List[Tuple[str, int]]]: Просееные варианты.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        sieved = []\n",
    "        for variant in variants: # нашли метку \"DEL\" -> удаляем вариант\n",
    "            if variant[-1][0] != \"DEL\":\n",
    "                sieved.append(variant)\n",
    "        return sieved\n",
    "    \n",
    "    def decode(self, result: List[List[Tuple[str, int]]]) -> Tuple[List[int], str]:\n",
    "        \"\"\"\n",
    "        Функция, которая преобразует вариант в список индексов, где должны стоять пробулы и полученный текст\n",
    "\n",
    "        Args:\n",
    "            result (List[List[Tuple[str, int]]]) : Список списков кортежей в формате (слово, позиция идущая после слова).\n",
    "\n",
    "        Returns:\n",
    "            Tuple[\n",
    "                List[int] : список индексов, где должны стоять пробелы\n",
    "                str       : запрос с расставленными пробелами\n",
    "            ]\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        indices = [x[1] for x in result]\n",
    "        text = [x[0] for x in result]\n",
    "        return indices, \" \".join(text).rstrip()\n",
    "    \n",
    "    def query_preprocessor(self, query : str) -> Tuple[List[str], List[Tuple[int, int]], List[str]]:\n",
    "        \"\"\"\n",
    "        Функция, ответственная за предобработку запроса.\n",
    "        Определяется наличие спец. символов их положение и тип.\n",
    "        Если необходимо - запрос разбивается на подзапросы\n",
    "\n",
    "        Args:\n",
    "            query (str): запрос.\n",
    "\n",
    "        Returns:\n",
    "        Tuple[\n",
    "            List[str]            : подзапросы\n",
    "            List[Tuple[int, int] : расположение спец. символов\n",
    "            List[str]]           : типы групп спец. символов\n",
    "        ]\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        query = query.lower()\n",
    "        idxs = []\n",
    "        types = []\n",
    "        \n",
    "        for i in range(len(query)): # классифицируем специальные символы по группам\n",
    "            if query[i] in self.digits:\n",
    "                types.append('d')\n",
    "                idxs.append(i)\n",
    "            if query[i] in self.punkt:\n",
    "                types.append('p')\n",
    "                idxs.append(i)\n",
    "            if query[i] in self.letters:\n",
    "                types.append('l')\n",
    "                idxs.append(i)\n",
    "\n",
    "\n",
    "        if idxs:\n",
    "            compressed_types = [types[0]] \n",
    "            start = end = idxs[0]\n",
    "            indices = []\n",
    "            for i in range(1, len(idxs)): # оюъединяем последовательные отрезки\n",
    "                if idxs[i] == idxs[i-1] + 1 and types[i] == types[i-1]:\n",
    "                    end = idxs[i]\n",
    "                else:\n",
    "                    indices += [(start, end)] if start != end else [(start, start)]\n",
    "                    start = end = idxs[i]\n",
    "            \n",
    "            indices += [(start, end)] if start != end else [(start, start)]\n",
    "\n",
    "            for i in range(1, len(types)): # объединяем типы в группы\n",
    "                if types[i] != compressed_types[-1]:\n",
    "                    compressed_types.append(types[i])\n",
    "\n",
    "            qs = []\n",
    "            first = query[:indices[0][0]]\n",
    "            if first != \"\":\n",
    "                qs.append(first)\n",
    "            for i in range(1, len(indices)): # разбиваем запросы на подзапросы\n",
    "                sub_query = query[indices[i-1][1] + 1:indices[i][0]]\n",
    "                if sub_query != \"\":\n",
    "                    qs.append(sub_query)\n",
    "            last = query[indices[-1][1] + 1:]\n",
    "            if last != \"\":\n",
    "                qs.append(last)\n",
    "            return qs, indices, compressed_types\n",
    "        \n",
    "        return [query], [], []\n",
    "\n",
    "    def query_processor(self, query: str, beam_width: int=5, is_subquery=False) -> List[List[Tuple[str, int]]]:\n",
    "\n",
    "        \"\"\"\n",
    "        Функция поиска слов с помощью Beam-Search\n",
    "\n",
    "        Args:\n",
    "            query      (str)  : запрос.\n",
    "            beam_width (int)  : количество кандидатов, которые остануться\n",
    "            is_subquery (bool): флаг, обозначающий, является ли запрос подзапросом\n",
    "\n",
    "        Returns:\n",
    "            List[List[Tuple[str, int]]]: Список списков кортежей в формате (слово, позиция идущая после слова)\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        if is_subquery == True and query in self.vocab: # Если прилетает подзапрос, пытаемся закрыть его подглядыванием в словарь\n",
    "            return [[(query, len(query))]]\n",
    "        \n",
    "        variants = self.get_next_variant(query, start=0, amount=5) # получение начальных вариантов\n",
    "        saved = []\n",
    "        i = 0\n",
    "\n",
    "        while i < 20 and saved == []: # аля Beam-Search для поиска вариантов с наименьшей перплексией\n",
    "\n",
    "            variants = self.extend_variants(query, variants)\n",
    "\n",
    "            variants = self.sieve(variants)\n",
    "            for variant in variants:\n",
    "                if variant[-1][1] == len(query): # Если мы дошли до конца запроса, перед нами ответ\n",
    "                    saved.append(variant)\n",
    "                    self.result = saved[0]\n",
    "                    return saved\n",
    "            variants = self.prune(variants, beam_width)\n",
    "            \n",
    "            i += 1\n",
    "        self.result = saved[0]\n",
    "        return saved\n",
    "    \n",
    "    def merge_subresults(self, subresults: List[List[List[Tuple[str, int]]]], indices: List[Tuple[int, int]] , types: List[str], query: str) -> List[int]:\n",
    "        \"\"\"\n",
    "        Функция, объединяющая ответы на подзапросы в единый ответ\n",
    "\n",
    "        Args:\n",
    "            subresults (List[List[List[Tuple[str, int]]]]) : Список ответов на подзапросы\n",
    "            indices (List[Tuple[int, int]])                : расположение спец. символов\n",
    "            types (List[str])                              : типы групп спец. символов\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            List[int] - Список индексов, на месте которых должен стоять пробел\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        i, j = 0, 0 \n",
    "        global_ptr = 0\n",
    "        rel_idxs = []\n",
    "        result = []\n",
    "\n",
    "        # получаем относительные позиции пробелов для каждого подзапроса\n",
    "        for subresult in subresults:\n",
    "            idxs, _ = self.decode(subresult[0])\n",
    "            rel_idxs.append(idxs)\n",
    "        \n",
    "        # обработка спец. символов и подзапросов\n",
    "        while i < len(indices) and j < len(rel_idxs):\n",
    "\n",
    "            rel_ptr = rel_idxs[j][0] if rel_idxs[j] else 0\n",
    "            \n",
    "            if indices[i][0] < global_ptr + rel_ptr: # перед нами спец. символ\n",
    "                \n",
    "                if indices[i][0] != 0 and (not result or result[-1] != indices[i][0]):\n",
    "                    result.append(indices[i][0])  # правый пробел\n",
    "                if indices[i][1] != len(query) and (not result or result[-1] != indices[i][1] + 1):\n",
    "                    result.append(indices[i][1] + 1)  # левый пробел\n",
    "                \n",
    "                global_ptr = indices[i][1] + 1 \n",
    "                i += 1\n",
    "            else:\n",
    "                # перед нами подзапрос\n",
    "                for rel_idx in rel_idxs[j]:\n",
    "                    abs_pos = rel_idx + global_ptr\n",
    "                    if not result or result[-1] != abs_pos:\n",
    "                        result.append(abs_pos)\n",
    "                \n",
    "                if rel_idxs[j]:\n",
    "                    global_ptr += rel_idxs[j][-1]\n",
    "                j += 1\n",
    "\n",
    "        # обработка оставшихся спец. символов\n",
    "        \n",
    "        while i < len(indices):\n",
    "            if indices[i][0] != 0 and (not result or result[-1] != indices[i][0]) and types[i] != 'p':\n",
    "                result.append(indices[i][0])\n",
    "            if indices[i][1] != len(query) and (not result or result[-1] != indices[i][1] + 1):\n",
    "                result.append(indices[i][1] + 1)\n",
    "            \n",
    "            global_ptr = indices[i][1] + 1\n",
    "            i += 1\n",
    "\n",
    "        # обработка оставшихся подзапросов\n",
    "        while j < len(rel_idxs):\n",
    "            for rel_idx in rel_idxs[j]:\n",
    "                abs_pos = rel_idx + global_ptr\n",
    "                if not result or result[-1] != abs_pos:\n",
    "                    result.append(abs_pos)\n",
    "            \n",
    "            if rel_idxs[j]:\n",
    "                global_ptr += rel_idxs[j][-1]\n",
    "            j += 1\n",
    "\n",
    "        # удаляем последний пробел, если он выходит за границы запроса\n",
    "        if result and result[-1] == len(query):\n",
    "            result.pop()\n",
    "        \n",
    "        # знаки пунктуации съедают пробел слева от себя\n",
    "        for i in range(len(query)):\n",
    "            if query[i] in self.punkt:\n",
    "                result.remove(i)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def impute_whitespaces(self, query: str) -> List[int]:\n",
    "        \"\"\"\n",
    "        Основная функция, которая просто запускает весь пайплайн.\n",
    "\n",
    "        Args:\n",
    "            query (str): запрос.\n",
    "\n",
    "        Returns:\n",
    "            List[int] : список индексов, где должны стоять пробелы\n",
    "        \n",
    "        \"\"\"\n",
    "        try:\n",
    "            # пробуем обработать запрос\n",
    "            query = query.lower()\n",
    "            subresults = []\n",
    "            subqueries, indices, types = self.query_preprocessor(query) # предобработка запроса\n",
    "            for subquery in subqueries:\n",
    "                subresults.append(self.query_processor(subquery, is_subquery=len(subqueries)>1 or indices != [])) # обработка запроса\n",
    "            \n",
    "            \n",
    "            if indices == []: # если запросов не было, то просто форматируем к ответу\n",
    "                return self.decode(subresults[0][0])[0]\n",
    "            \n",
    "            result = self.merge_subresults(subresults, indices, types, query) # если были подзапросы - объединяем и форматируем\n",
    "            return result\n",
    "        except: # если не получилось обработать запрос, выдаем пустой список и выводим запрос, вызваввший ошибку \n",
    "            print(query)\n",
    "            return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8e18657b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = PerplexityImputer(\n",
    "    vocab,\n",
    "    model,\n",
    "    tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "846b4ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(data_df, imputer, path: str = \"submission.csv\") -> None:\n",
    "    data_df[\"predicted_positions\"] = data_df.apply(lambda row: imputer.impute_whitespaces(row[\"text_no_spaces\"]), axis=1)\n",
    "    submission = data_df.drop(columns=[\"text_no_spaces\"])\n",
    "    submission[\"predicted_positions\"] = submission[\"predicted_positions\"].astype(str)\n",
    "    submission.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04e516f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "якалендарьперевернул\n",
      "твоядочьимойсын—\n",
      "аркадийукупник—кусающийлжец\n",
      "мыпьемчайвстарыхквартирах,\n",
      "однасемья,двесемьи,трисемьи...\n",
      "рядомсметро,центр...\n",
      "всеговорят,чтомыв-месте...\n",
      "стой!опаснаязона!работамозга!..\n",
      "...годамидолгими.\n",
      "...ночамитемными.\n",
      "искалатебяночами,чами,чами,чами...\n",
      "абезмузыкинамирусмертьнекрасна,\n",
      "счастьеданоповстречатьильбедуеще\n",
      "приказаверитьвчудеса\n",
      "колотьустала.\n",
      "волкиуходят...\n",
      "билетводинконец—\n",
      "ялюблюслушатьпесни,икостранюхатьдым\n",
      "сердцебьетсядругимивершинами\n",
      "сигареткаосталасьвсегоодна,нуладнона...\n",
      "замыкание+бах!ивсталитрамваи\n",
      "онаскажет:'а,вообще,володька,хреново!'\n",
      "пришёлкколдунье:'ану-канаколдуймне!'\n",
      "'легко,мойхороший,толькохлопнувладоши,\n",
      "ималенькаяжизньвнутринеёоборвётся.'\n",
      "привидений,говорят:'небыватьпреступленью'\n",
      "сдавайся,ведьма–«ночнойдозор».\n",
      "потёмнымулицамлетит«ночнойдозор».\n",
      "носила«иного»вантоновомвзоре,\n",
      "азначит,онбудетработатьв«дозоре».\n",
      "ещебы,абыкогонеберутвзамминистры.\n",
      "нозлойзавулониз«дозорадневного»\n",
      "сделализсынаантона«иного».\n",
      "иновоеутро,иновое'неттебя...'\n",
      "'друзьяхотятпокушать,пойдём,приятель,влес!'\n",
      "телефонныйзвонок,каккоманда'вперед!'\n",
      "аещекрасивыйгалстукуменя.\n"
     ]
    }
   ],
   "source": [
    "create_submission(data_df, pi, \"submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4da2dd",
   "metadata": {},
   "source": [
    "Алгоритм отказался размечать 37 сэмплов из 1004, о причинах подумаем позднее..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937c2ef0",
   "metadata": {},
   "source": [
    "Засылаем сабмит, получем скор: Your Mean F1 = **79.266%** 🎉🎉🎉"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15778d62",
   "metadata": {},
   "source": [
    "## Артефакты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c03d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_imputer_max(query : str, vocab, vc=10, bl=10) -> list[int]:\n",
    "    # Ф-ция успешно обрабатывает только часть простых запросов и является просто proof-of-concept!\n",
    "    try:\n",
    "        print(query)\n",
    "        variants = []\n",
    "        whitespaces = []\n",
    "        buffer = \"\"\n",
    "        result = []\n",
    "        parsed_index = 0\n",
    "        \n",
    "        query = query.lower()\n",
    "        i = 0\n",
    "        while i < len(query):\n",
    "            buffer += query[i]\n",
    "            \n",
    "            if buffer in vocab:\n",
    "                print(buffer)\n",
    "                variants.append(buffer)\n",
    "\n",
    "            if len(variants) == vc or len(buffer) == bl or i + 1 == len(query):\n",
    "                print(f\"\\n\\nbuffer overflow : {len(buffer) == bl}\\nvariant overflow : {len(variants) == vc}\\n\\n\")\n",
    "                print(variants)\n",
    "                result.append(max(list(zip(map(len, variants), variants)), key=lambda x: x[0])[1])\n",
    "                parsed_index += len(result[-1])\n",
    "                whitespaces.append(parsed_index)\n",
    "                buffer = \"\"\n",
    "                variants = []\n",
    "                i = parsed_index\n",
    "                print(f\"{result[-1]} : saved\")\n",
    "                continue\n",
    "\n",
    "\n",
    "            i += 1\n",
    "    except:\n",
    "        return \" \""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
